{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Group Assignment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot settings\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "colours = ['#1F77B4', '#FF7F0E', '#2CA02C', '#DB2728', '#9467BD', '#8C564B', '#E377C2','#7F7F7F', '#BCBD22', '#17BECF']\n",
    "crayon = ['#4E79A7','#F28E2C','#E15759','#76B7B2','#59A14F', '#EDC949','#AF7AA1','#FF9DA7','#9C755F','#BAB0AB']\n",
    "sns.set_palette(colours)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/ProjectTrain.csv',index_col='SK_ID_CURR')\n",
    "train_bur = pd.read_csv('Data/ProjectTrain_Bureau.csv',index_col='SK_ID_CURR')\n",
    "test = pd.read_csv('Data/ProjectTest.csv',index_col='Index_ID')\n",
    "test_bur = pd.read_csv('Data/ProjectTest_Bureau.csv',index_col='Index_ID')\n",
    "\n",
    "# 50% + filtering numerical transform\n",
    "train_trans_partial = pd.read_csv('Data/train_clean_trans.csv', index_col='SK_ID_CURR')\n",
    "test_trans_partial = pd.read_csv('Data/test_clean_trans.csv', index_col='Index_ID')\n",
    "# # full set numerical transform\n",
    "train_trans_full = pd.read_csv('Data/train_trans_fullset.csv',index_col='SK_ID_CURR')\n",
    "test_trans_full = pd.read_csv('Data/test_trans_fullset.csv',index_col='Index_ID')\n",
    "# full imputed \n",
    "X_train_filled = pd.read_csv('Data/X_train_filled.csv',index_col='SK_ID_CURR')\n",
    "X_test_filled = pd.read_csv('Data/X_test_filled.csv',index_col='Index_ID')\n",
    "# # full imputed \n",
    "X_train_oh = pd.read_csv('Data/X_train_oh.csv',index_col='SK_ID_CURR')\n",
    "X_test_oh = pd.read_csv('Data/X_test_oh.csv',index_col='Index_ID')\n",
    "# Merged Data\n",
    "train_join = train.merge(train_bur, how='left', left_on='SK_ID_CURR', right_on='SK_ID_CURR')\n",
    "test_join = test.merge(test_bur, how='left', left_on='Index_ID', right_on='Index_ID')\n",
    "train_join_trans = pd.read_csv('Data/train_join_trans.csv',index_col='SK_ID_CURR')\n",
    "test_join_trans = pd.read_csv('Data/test_join_trans.csv',index_col='Index_ID')\n",
    "pearson_corr_features_join = pd.read_csv('Data/feature_join_corr.csv',index_col='index')\n",
    "test_join.loc[test_join['CREDIT_ACTIVE'] == 'Bad debt', 'CREDIT_ACTIVE'] = 'Closed'\n",
    "## well processed data\n",
    "# original\n",
    "X_train_filled_original = pd.read_csv('Data/Original_X_train.csv', index_col='SK_ID_CURR')\n",
    "y_train_sub = pd.read_csv('Data/Original_y_train.csv', index_col='SK_ID_CURR')\n",
    "# hold-out set\n",
    "X_validate_filled_original = pd.read_csv('Data/Original_X_validate.csv', index_col='SK_ID_CURR')\n",
    "y_validate_sub = pd.read_csv('Data/Original_y_validate.csv', index_col='SK_ID_CURR')\n",
    "# test set\n",
    "X_test_filled_original = pd.read_csv('Data/Original_X_test.csv',index_col='Index_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Filtering for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Fold Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_change(train):\n",
    "    # data preparation\n",
    "    train_target = train['TARGET'].copy # training target\n",
    "    train_features = train.iloc[:,1:].copy() # training features\n",
    "    # partition feature into two groups by target & Mean calc\n",
    "    train_feature_oneMean = train_features.loc[train['TARGET'] == 1,:].mean()\n",
    "    train_feature_zeroMean = train_features.loc[train['TARGET'] == 0,:].mean()\n",
    "    # fold change calculation\n",
    "    train_fold_change = pd.DataFrame(np.absolute(np.log2(np.divide(train_feature_oneMean, train_feature_zeroMean))))\n",
    "    train_fold_change.columns = ['Mean'] # rename mean division result column\n",
    "    train_fold_change_sort = train_fold_change.sort_values(by=['Mean'],ascending=False) # fold change calculation\n",
    "    \n",
    "    return train_fold_change_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Pearson's Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pearson_corr(train):\n",
    "    # data preparation\n",
    "    train_t = train.copy()\n",
    "    train_features = train_t.drop(columns=['TARGET']).copy() # training features\n",
    "    # corr calculation\n",
    "    train_corr = pd.DataFrame((train_features.corrwith(train['TARGET'])).abs())\n",
    "    train_corr.columns = ['Corr']\n",
    "    train_corr_sort = train_corr.sort_values(by=['Corr'],ascending=False)\n",
    "    \n",
    "    return train_corr_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Filter Out Columns with 50%+ Missing Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MissingValFilter(train, test):\n",
    "    # pass value to new vars\n",
    "    train_new = train.copy()\n",
    "    test_new = test.copy()\n",
    "    # the missing proportion in each column\n",
    "    miss_propor = train_new.isna().sum()/train.shape[0]\n",
    "    # filter out the column if the missing proportion is larger than 45%\n",
    "    for i in range(0,train_new.shape[1]):\n",
    "        if miss_propor[i] > 0.45:\n",
    "            del train_new[miss_propor.index[i]]\n",
    "            del test_new[miss_propor.index[i]]\n",
    "    # filter out columns with only 1 level\n",
    "    # 1 level index retrive\n",
    "    level_summary = train_new.nunique()\n",
    "    one_level_index = level_summary[level_summary==1].index\n",
    "    # 1 level column filtering\n",
    "    if one_level_index != 'nan':\n",
    "        for i in range(0,one_level_index.shape[0]):\n",
    "            del train_new[one_level_index[i]]\n",
    "            del test_new[one_level_index[i]]\n",
    "    return train_new, test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Nominal Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumericalTransform(train_clean, test_clean):  \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # pass value to new pars\n",
    "    train_clean_trans_tmp = train_clean.copy()\n",
    "    test_clean_trans_tmp = test_clean.copy()\n",
    "    # retrieve column types\n",
    "    types = train_clean_trans_tmp.dtypes\n",
    "    types_num = types[types != 'object'].index # numerical type colnames\n",
    "    types_cat = types[types == 'object'].index # numerical type colnames\n",
    "    # numerical & categorial columns partition\n",
    "    # numerical cols\n",
    "    train_clean_trans_num = train_clean_trans_tmp[types_num].copy() # train\n",
    "    test_clean_trans_num = test_clean_trans_tmp[types_num[1:]].copy() # test\n",
    "    # cat cols\n",
    "    train_clean_trans_cat = train_clean_trans_tmp[types_cat].copy() # train\n",
    "    test_clean_trans_cat = test_clean_trans_tmp[types_cat].copy() # test\n",
    "    \n",
    "    # Categorical col encoding - label encoder\n",
    "    for i in range(0,types_cat.shape[0]):\n",
    "        le = LabelEncoder()\n",
    "        # fit with the desired col, col in position 0 for this example\n",
    "        fit_by = pd.Series([i for i in train_clean_trans_tmp[types_cat[i]].unique() if type(i) == str]) # train\n",
    "        le.fit(fit_by)\n",
    "        # Set transformed col leaving np.NaN as they are\n",
    "        train_clean_trans_cat[types_cat[i]] = train_clean_trans_tmp[types_cat[i]].apply(lambda x: le.transform([x])[0] if type(x) == str else x) # train\n",
    "        test_clean_trans_cat[types_cat[i]] = test_clean_trans_tmp[types_cat[i]].apply(lambda x: le.transform([x])[0] if type(x) == str else x) # test\n",
    "        \n",
    "    # cocat Numerical & Categorial Cols tgt\n",
    "    # train\n",
    "    frame = [train_clean_trans_num,train_clean_trans_cat]\n",
    "    train_clean_trans = pd.concat(frame,axis=1)\n",
    "    # test\n",
    "    frame = [test_clean_trans_num,test_clean_trans_cat]\n",
    "    test_clean_trans = pd.concat(frame,axis=1)\n",
    "    \n",
    "    return train_clean_trans, test_clean_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Train Validation Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValidateSample(train,y_train):\n",
    "    # pass values to temp pars\n",
    "    train_t = train.copy()\n",
    "    y_train_t = y_train.copy()\n",
    "    # sample the selected training set\n",
    "    train_ex = train_t.sample(frac = 0.2185, replace = False, random_state = 1) # features\n",
    "    y_train_ex = y_train[train_ex.index] # target\n",
    "    # sample the selected validation set\n",
    "    oppSubSample_index = train_t.index.isin(train_ex.index)\n",
    "    subSample_val = train_t[~oppSubSample_index] # rebulid validation subsample\n",
    "    validate_ex = subSample_val.sample (frac = 0.120, replace = False, random_state = 1)\n",
    "    y_validate_ex = y_train[validate_ex.index]\n",
    "    \n",
    "    return train_ex, y_train_ex, validate_ex, y_validate_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Categorical String Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(X_train_filled, X_validate_filled,X_test_filled, train, pearson_corr_features):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    # pass value to new pars\n",
    "    X_train_filled_t = X_train_filled.copy()\n",
    "    X_validate_filled_t = X_validate_filled.copy()\n",
    "    X_test_filled_t = X_test_filled.copy()\n",
    "    train_t = train.copy()\n",
    "    pearson_corr_features_t = pearson_corr_features.copy()\n",
    "    pearson_corr_features_t = pearson_corr_features_t.drop(labels=['EXT_SOURCE_1'])\n",
    "    # retrieve column types\n",
    "    types = train_t.dtypes\n",
    "    types_cat = types[types=='object'].index # categorial type colnames\n",
    "    unique_count = train_t.nunique()\n",
    "    types_cat2 = unique_count[unique_count<=70].index # category<=70 all count in categorical type \n",
    "    pearson_corr_features_t = pearson_corr_features_t.loc[pearson_corr_features_t['Corr']>=0.01,:].index\n",
    "    # extract binary and categorial features\n",
    "    features_ex_cat=[]\n",
    "    features_ex_num=[]\n",
    "    for i in range(0,len(pearson_corr_features_t)):\n",
    "        if ((pearson_corr_features_t[i] in types_cat.unique())|(pearson_corr_features_t[i] in types_cat2.unique())):\n",
    "            features_ex_cat.append(pearson_corr_features_t[i])\n",
    "        else:\n",
    "            features_ex_num.append(pearson_corr_features_t[i])\n",
    "            \n",
    "    # convert to object type\n",
    "    for col in range(0,len(features_ex_cat)):\n",
    "        X_train_filled_t[features_ex_cat[col]] = X_train_filled_t[features_ex_cat[col]].astype(str)\n",
    "        X_validate_filled_t[features_ex_cat[col]] = X_validate_filled_t[features_ex_cat[col]].astype(str)\n",
    "        X_test_filled_t[features_ex_cat[col]] = X_test_filled_t[features_ex_cat[col]].astype(str)\n",
    "    \n",
    "    return X_train_filled_t, X_validate_filled_t, X_test_filled_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# MICE Imputer\n",
    "def MiceImpute(train_clean_trans, validate_clean_trans, test_clean_trans):\n",
    "    from fancyimpute import IterativeImputer\n",
    "    \n",
    "    imputer = IterativeImputer(n_iter=5, sample_posterior=True, random_state=1)\n",
    "    imputer.fit(train_clean_trans)\n",
    "    X_train_filled_mice = pd.DataFrame(imputer.transform(train_clean_trans)) # train\n",
    "    X_validate_filled_mice = pd.DataFrame(imputer.transform(validate_clean_trans)) # validate\n",
    "    X_test_filled_mice = pd.DataFrame(imputer.transform(test_clean_trans)) # test\n",
    "    # rename the column names\n",
    "    X_train_filled_mice.columns = train_clean_trans.columns # train\n",
    "    X_validate_filled_mice.columns = validate_clean_trans.columns # validate\n",
    "    X_test_filled_mice.columns = test_clean_trans.columns # test\n",
    "    # rename the row\n",
    "    X_train_filled_mice.index = train_clean_trans.index # train\n",
    "    X_validate_filled_mice.index = validate_clean_trans.index # validate\n",
    "    X_test_filled_mice.index = test_clean_trans.index # test\n",
    "    \n",
    "    return X_train_filled_mice, X_validate_filled_mice, X_test_filled_mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode Imputer\n",
    "def SimpleImpute(train_clean_trans, test_clean_trans,stra):\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    imputer = Imputer(strategy=stra)\n",
    "    X_train_filled_mode = imputer.fit_transform(train_clean_trans) # train\n",
    "    X_test_filled_mode = pd.DataFrame(imputer.transform(test_clean_trans)) # test\n",
    "    # rename the column names\n",
    "    X_train_filled_mode.columns = train_clean_trans.columns # train\n",
    "    X_test_filled_mode.columns = test_clean_trans.columns # test\n",
    "    # rename the row\n",
    "    X_train_filled_mode.index = train_clean_trans.index # train\n",
    "    X_test_filled_mode.index = test_clean_trans.index # test\n",
    "    \n",
    "    return X_train_filled_mode, X_test_filled_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Final Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_impute(train, validate, test, pearson_corr_features):\n",
    "    train_t = train.copy()\n",
    "    validate_t = validate.copy()\n",
    "    test_t = test.copy()\n",
    "    # MNAR Colnames\n",
    "    mnar_colnames = pd.read_csv('Data/col_mnar.csv',index_col='index')\n",
    "    mnar_colnames = (mnar_colnames.loc[mnar_colnames['val'] == -1,:]).index\n",
    "    # extract feature with 1%+ features\n",
    "    pearson_corr_features = pearson_corr_features.loc[pearson_corr_features['Corr']>=0.01,:].index\n",
    "    # Total extracted data\n",
    "    train_t_ex = train_t[pearson_corr_features].copy()\n",
    "    validate_t_ex = validate_t[pearson_corr_features].copy()\n",
    "    test_t_ex = test_t[pearson_corr_features].copy()\n",
    "    # MNAR\n",
    "    train_t_ex[mnar_colnames] = train_t_ex[mnar_colnames].fillna(100)\n",
    "    validate_t_ex[mnar_colnames] = validate_t_ex[mnar_colnames].fillna(100)\n",
    "    test_t_ex[mnar_colnames] = test_t_ex[mnar_colnames].fillna(100)\n",
    "    # drop EXT_SOURCE_1 col since too much missing \n",
    "    train_t_ex = train_t_ex.drop(columns=['EXT_SOURCE_1'])\n",
    "    validate_t_ex = validate_t_ex.drop(columns=['EXT_SOURCE_1'])\n",
    "    test_t_ex = test_t_ex.drop(columns=['EXT_SOURCE_1'])\n",
    "    # MICE\n",
    "    X_train_filled_mice, X_validate_filled_mice, X_test_filled_mice = MiceImpute(train_t_ex, validate_t_ex, test_t_ex)\n",
    "    \n",
    "    return X_train_filled_mice, X_validate_filled_mice, X_test_filled_mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Factor Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 FAMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def princeFAMD (train, validate, test, n_comp, n_iter):\n",
    "    from prince import FAMD\n",
    "    \n",
    "    famd = FAMD(n_components=n_comp, n_iter=n_iter, copy=True, engine='auto', random_state=4)\n",
    "    # fit transform\n",
    "    train_trans = famd.fit_transform(train)\n",
    "    validate_trans = famd.fit_transform(validate)\n",
    "    test_trans = famd.fit_transform(test)\n",
    "\n",
    "    return train_trans, validate_trans, test_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 FAMD Optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def princeFAMD_Opt (train, n_comp, n_iter):\n",
    "    from prince import FAMD\n",
    "\n",
    "    famd = FAMD(n_components=n_comp, n_iter=n_iter, copy=True, engine='auto', random_state=4)\n",
    "    # fit transform\n",
    "    train_trans = famd.fit_transform(train)\n",
    "\n",
    "    return famd, train_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Benchmark KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def BmModel(X_train_filled, y_train):\n",
    "    \n",
    "# #     # Benchmark Knn\n",
    "#     model_knn = KNeighborsClassifier()\n",
    "#     # cross validation score\n",
    "#     scores_knn = cross_val_score(model_knn, X_train_filled, y_train, cv=5, n_jobs=-1, scoring = 'accuracy')\n",
    "#     cv_score_knn = scores_knn.mean() # avg cv score\n",
    "#     print('Cross Validation Score in knn Benchmark:', cv_score_knn.round(3), '\\n')\n",
    "\n",
    "    # Benchmark RandomForest\n",
    "    model_rf = RandomForestClassifier()\n",
    "#     scores_rf_raw = cross_val_score(model_rf, X_train_raw, y_train, cv=5, n_jobs=-1, scoring = 'roc_auc')\n",
    "    scores_rf = cross_val_score(model_rf, X_train_filled, y_train, cv=5, n_jobs=-1, scoring = 'roc_auc')\n",
    "    # avg cv score\n",
    "#     cv_score_rf_raw = scores_rf_raw.mean()\n",
    "    cv_score_rf = scores_rf.mean() \n",
    "#     print('Cross Validation Score in random forest Benchmark (Raw Data):', cv_score_rf_raw.round(3), '\\n')\n",
    "    print('Cross Validation Score in random forest Benchmark:', cv_score_rf.round(3), '\\n')\n",
    "    \n",
    "    return model_rf, cv_score_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_cv(X_train, y_train, max_depth, min_samples_leaf):\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "    \n",
    "    brf_classifier = BalancedRandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=1)\n",
    "    tuning_parameters = {\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_leaf' : min_samples_leaf,\n",
    "#     'sampling_strategy': ['majority','not minority'],\n",
    "#     'replacement': [True, False],\n",
    "#     'class_weight': [None, 'balanced']\n",
    "    }\n",
    "    \n",
    "    brf_cls_search = RandomizedSearchCV(brf_classifier, tuning_parameters, cv= 5, return_train_score=True, scoring='f1')\n",
    "    brf_cls_search.fit(X_train, y_train)\n",
    "    brf_cls_search_best = brf_cls_search.best_estimator_\n",
    "    print('Best parameters found by grid search:', brf_cls_search.best_params_, '\\n')\n",
    "    \n",
    "    return brf_cls_search, brf_cls_search_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(object, features, target):\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score\n",
    "    \n",
    "    y_pred = object.predict(features)\n",
    "    # confusion matrix\n",
    "    confusion =(confusion_matrix(target, y_pred)/target.shape).round(3)\n",
    "    # performance report\n",
    "    performance =classification_report(target, y_pred, digits = 3)\n",
    "    # accuracy score\n",
    "    accuracy = accuracy_score(target, y_pred).round(3)\n",
    "    # AUC score\n",
    "    auc = roc_auc_score(target, y_pred).round(3)\n",
    "    \n",
    "    return y_pred, confusion, performance, accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Data Preprocessomg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SET 1: Original set generation\n",
    "'''\n",
    "# ## STEP 1: extract TARGET N Numerical Transform\n",
    "# y_train = train['TARGET'] # extract the training target\n",
    "# # train_trans_full, test_trans_full = NumericalTransform(train, test) #numerical transform of the whole dataset\n",
    "# ## STEP 2: resample of the sub training n validation set\n",
    "# train_sub, y_train_sub, validate_sub, y_validate_sub = trainValidateSample(train_trans_full,y_train)\n",
    "# ## STEP 3: drop the target column in feature set\n",
    "# train_sub, validate_sub = train_sub.drop(columns=['TARGET']), validate_sub.drop(columns=['TARGET'])\n",
    "# ## STEP 4: impute missing value\n",
    "# X_train_filled_original, X_validate_filled_original, X_test_filled_original = MiceImpute(train_sub, validate_sub, test_trans_full)\n",
    "# ## STEP 5: output the data file\n",
    "# # training set\n",
    "# X_train_filled_original.to_csv('Data/Original_X_train.csv')\n",
    "# y_train_sub.to_csv('Data/Original_y_train.csv')\n",
    "# # hold-out set\n",
    "# X_validate_filled_original.to_csv('Data/Original_X_validate.csv')\n",
    "# y_validate_sub.to_csv('Data/Original_y_validate.csv')\n",
    "# # test set\n",
    "# X_test_filled_original.to_csv('Data/Original_X_test.csv')\n",
    "\n",
    "\n",
    "'''\n",
    "SET 2: Without Extra Features Set Generation\n",
    "'''\n",
    "## STEP 4: Pearson Correlation Calculation\n",
    "# pearson_corr_features = Pearson_corr(train_trans_full)\n",
    "## STEP 5: impute missing values\n",
    "# X_train_filled_ne, X_validate_filled_ne, X_test_filled_ne= final_impute(train_sub, validate_sub, test_trans_full, pearson_corr_features)\n",
    "# ## STEP 6: categorial string transformation\n",
    "X_train_oh_ne, X_validate_oh_ne, X_test_oh_ne = oneHotEncoding(X_train_filled_ne, X_validate_filled_ne,X_test_filled_ne, train, pearson_corr_features)\n",
    "# ## STEP 7: FAMD factor compression\n",
    "# X_train_famd_ne, X_validate_famd_ne, X_test_famd_ne = princeFAMD(X_train_oh_ne, X_validate_oh_ne, X_test_oh_ne, 40, 5)\n",
    "# ## STEP 8: output the data file\n",
    "# # training set\n",
    "# X_train_famd_ne.to_csv('Data/ne_X_train.csv')\n",
    "# # hold-out set\n",
    "# X_validate_famd_ne.to_csv('Data/ne_X_validate.csv')\n",
    "# # test set\n",
    "# X_test_famd_ne.to_csv('Data/ne_X_test.csv')\n",
    "\n",
    "\n",
    "'''\n",
    "SET 3: With Extra Features Set Generation\n",
    "'''\n",
    "# ## STEP 1: Numerical Transform\n",
    "# train_trans_join, test_trans_join = NumericalTransform(train_join, test_join) #numerical transform of the whole dataset\n",
    "# ## STEP 2: pearson corr calc\n",
    "# pearson_corr_features_join = pd.read_csv('Data/feature_join_corr.csv',index_col='index')\n",
    "# ## STEP 3: resample of the sub training n validation set\n",
    "# train_join_sub, y_train_join_sub, validate_join_sub, y_validate_sub = trainValidateSample(train_trans_join,y_train)\n",
    "# ## STEP 4:  drop the target column in feature set\n",
    "# train_join_sub, validate_join_sub = train_join_sub.drop(columns=['TARGET']), validate_join_sub.drop(columns=['TARGET'])\n",
    "# ## STEP 5: impute missing values\n",
    "# X_train_filled_we, X_validate_filled_we, X_test_filled_we= final_impute(train_join_sub, validate_join_sub, test_trans_join, pearson_corr_features_join)\n",
    "# ## STEP 6: categorial string transformation\n",
    "# X_train_oh_we, X_validate_oh_we, X_test_oh_we = oneHotEncoding(X_train_filled_we, X_validate_filled_we,X_test_filled_we, train, pearson_corr_features_join)\n",
    "# ## STEP 7: FAMD factor compression\n",
    "# X_train_famd_we, X_validate_famd_we, X_test_famd_we = princeFAMD(X_train_oh_we, X_validate_oh_we, X_test_oh_we, 40, 5)\n",
    "# ## STEP 8: output the data file\n",
    "# # training set\n",
    "# X_train_famd_we.to_csv('Data/we_X_train.csv')\n",
    "# y_train_join_sub.to_csv('Data/we_y_train.csv')\n",
    "# ## hold-out set\n",
    "# # X_validate_famd_we.to_csv('Data/we_X_validate.csv')\n",
    "# y_validate_sub.to_csv('Data/we_y_validate.csv')\n",
    "# ## test set\n",
    "# # X_test_famd_we.to_csv('Data/we_X_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
